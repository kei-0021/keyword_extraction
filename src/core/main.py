import base64
import os
import tempfile
from collections import Counter

import streamlit as st
from dotenv import load_dotenv

from src.core.notion_handler import fetch_good_things
from src.core.sheets_writer import connect_to_sheet, write_word_count
from src.core.word_analyser import analyse_word
from src.utils.supabase import get_supabase_client

TOP_N = 5  # é »å‡ºå˜èªã®ä¸Šä½ã‹ã‚‰æ•°ãˆã¦ä½•å€‹ã‚’è¡¨ç¤ºã™ã‚‹ã‹
DAY_LINIT = 30  # éå»ä½•æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹


def main() -> Counter:
    IS_RENDER = os.getenv("RENDER") == "true"
    IS_DEV = not IS_RENDER  # RENDERãŒtrueã§ãªã‘ã‚Œã°é–‹ç™ºç’°å¢ƒ

    if not IS_DEV:
        # Renderç’°å¢ƒï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã—ã€jsonã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼‰
        NOTION_TOKEN = os.getenv("NOTION_TOKEN")
        DATABASE_ID = os.getenv("DATABASE_ID")

        b64_creds = os.getenv("GOOGLE_CREDENTIALS_JSON")
        if not b64_creds:
            raise ValueError("GOOGLE_CREDENTIALS_JSON ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        decoded = base64.b64decode(b64_creds)
        with tempfile.NamedTemporaryFile(
            mode="w+b", delete=False, suffix=".json"
        ) as tmp:
            tmp.write(decoded)
            GOOGLE_CREDENTIALS_JSON = tmp.name  # ğŸ‘ˆ tempãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ã‚»ãƒƒãƒˆ
    else:
        # é–‹ç™ºç’°å¢ƒç”¨è¨­å®š
        load_dotenv(dotenv_path="config/.env")
        NOTION_TOKEN = os.getenv("NOTION_TOKEN")
        DATABASE_ID = os.getenv("DATABASE_ID")
        GOOGLE_CREDENTIALS_JSON = os.getenv("GOOGLE_CREDENTIALS_PATH")

    # å¿…é ˆã®ç’°å¢ƒå¤‰æ•°ãŒæƒã£ã¦ã„ã‚‹ã‹ç¢ºèª
    if not all([NOTION_TOKEN, DATABASE_ID, GOOGLE_CREDENTIALS_JSON]):
        raise ValueError("ç’°å¢ƒå¤‰æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # Notionã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    all_text: str = fetch_good_things(NOTION_TOKEN, DATABASE_ID, DAY_LINIT)

    if IS_DEV:
        # é–‹ç™ºç”¨ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚‹ stop_words.txt ã‚’èª­ã‚€
        with open("custom_dict/stop_words.txt", encoding="utf-8") as f:
            stop_words_set = set(line.strip() for line in f if line.strip())
    else:
        # æœ¬ç•ªã¯Supabaseã‹ã‚‰å–å¾—
        supabase = get_supabase_client()
        user_id = st.session_state.user.id
        response = (
            supabase.table("stop_words").select("word").eq("user_id", user_id).execute()
        )
        stop_words_set = set(item["word"] for item in response.data)

    # è§£æå‡¦ç†ã¯å…±é€š
    word_count: Counter = analyse_word(all_text, "custom_dict/user.dic", stop_words_set)

    # Google Sheetsæ¥ç¶šã¨æ›¸ãè¾¼ã¿
    worksheet = connect_to_sheet(GOOGLE_CREDENTIALS_JSON, "Keyword Extraction")
    write_word_count(worksheet, word_count, TOP_N)

    # é–‹ç™ºç’°å¢ƒãªã‚‰ãƒ­ã‚°ã«é »å‡ºå˜èªã‚’å‡ºåŠ›
    if IS_DEV:
        print(word_count.most_common(TOP_N))

    return word_count


if __name__ == "__main__":
    main()

"""ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚"""

import base64
import os
import tempfile
from collections import Counter

import kaleido
import MeCab
import streamlit as st
from dotenv import load_dotenv

from src.core.csv_to_dic import (
    build_user_dic_from_csv_data,
    build_user_dic_from_local_file,
)
from src.core.plot import generate_bar_chart
from src.core.word_analyser import analyse_word
from src.services.notion_handler import fetch_good_things
from src.services.supabase_client import get_supabase_client

TOP_N = 5  # é »å‡ºå˜èªã®ä¸Šä½ã‹ã‚‰æ•°ãˆã¦ä½•å€‹ã‚’è¡¨ç¤ºã™ã‚‹ã‹
DAY_LIMIT = 30  # éå»ä½•æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹


@st.cache_resource
def get_tagger(custom_dict_path: str) -> MeCab.Tagger:
    """MeCab Tagger ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å–å¾—ã™ã‚‹"""
    print("ğŸŸ¡ MeCab.Tagger ã‚’æ–°è¦ç”Ÿæˆã—ã¾ã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰")
    return MeCab.Tagger(
        f"-r /etc/mecabrc -d /var/lib/mecab/dic/ipadic-utf8 -u {custom_dict_path}"
    )


def run_keyword_extraction() -> Counter:
    """Notionãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã€åè©é »åº¦ã®ã‚«ã‚¦ãƒ³ãƒˆçµæœã‚’è¿”ã™ã€‚

    - Renderç’°å¢ƒã‹ã©ã†ã‹ã§å‡¦ç†ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹
    - Supabaseä¸Šã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‚’ä½¿ç”¨
    - Renderç’°å¢ƒã§ã¯Supabaseã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‚’å–å¾—ã—MeCabè¾æ›¸ã‚’ä¸€æ™‚ç”Ÿæˆ
    - ãã‚Œä»¥å¤–ã¯é–‹ç™ºç’°å¢ƒã¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«è¾æ›¸ã‚’åˆ©ç”¨ã—ã€ãªã‘ã‚Œã°ãƒ“ãƒ«ãƒ‰ã™ã‚‹
    """

    IS_RENDER = os.getenv("RENDER") == "true"

    if IS_RENDER:
        # Renderç’°å¢ƒï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã—ã€jsonã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼‰
        NOTION_TOKEN = os.getenv("NOTION_TOKEN")
        DATABASE_ID = os.getenv("DATABASE_ID")

        b64_creds = os.getenv("GOOGLE_CREDENTIALS_JSON")
        if not b64_creds:
            raise ValueError("GOOGLE_CREDENTIALS_JSON ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        decoded = base64.b64decode(b64_creds)
        with tempfile.NamedTemporaryFile(
            mode="w+b", delete=False, suffix=".json"
        ) as tmp:
            tmp.write(decoded)
            GOOGLE_CREDENTIALS_JSON = tmp.name

        supabase = get_supabase_client()
        try:
            user_id = st.session_state.user.id
        except Exception:
            user_id = os.getenv("USER_ID") or ""

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¯Supabaseã‹ã‚‰å–å¾—
        response = (
            supabase.table("stop_words").select("word").eq("user_id", user_id).execute()
        )
        stop_words_set = set(item["word"] for item in response.data)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‚’Supabaseã‹ã‚‰å–å¾—ã—ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã§MeCabè¾æ›¸ç”Ÿæˆ
        print("build_user_dic_from_db: start")
        response = (
            supabase.table("user_dict")
            .select("word, part_of_speech, reading, pronunciation")
            .eq("user_id", user_id)
            .execute()
        )
        entries = response.data or []
        print(f"DBã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸å–å¾—ä»¶æ•°: {len(entries)}")

        if not entries:
            print("ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ãŒç©ºãªã®ã§ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ")
            temp_csv = tempfile.NamedTemporaryFile(
                mode="w", encoding="utf-8", delete=False, suffix=".csv"
            )
            temp_csv.write("")
            temp_csv.close()

            temp_dic = tempfile.NamedTemporaryFile(delete=False, suffix=".dic")
            temp_dic.close()
            custom_dict_path = temp_dic.name
        else:
            csv_data = "\n".join(
                f"{e['word']},{e['part_of_speech']},{e['reading']},{e['pronunciation']}"
                for e in entries
            )
            custom_dict_path = build_user_dic_from_csv_data(
                csv_data, dic_dir="/usr/share/mecab/dic/ipadic"
            )

    else:
        # ãã‚Œä»¥å¤–ï¼ˆé–‹ç™ºç’°å¢ƒï¼‰ï¼š.envèª­ã¿è¾¼ã¿ã€ãƒ­ãƒ¼ã‚«ãƒ«è¾æ›¸åˆ©ç”¨
        load_dotenv(dotenv_path="config/.env")
        NOTION_TOKEN = os.getenv("NOTION_TOKEN")
        DATABASE_ID = os.getenv("DATABASE_ID")
        GOOGLE_CREDENTIALS_JSON = os.getenv("GOOGLE_CREDENTIALS_PATH")

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        with open("custom_dict/stop_words.txt", encoding="utf-8") as f:
            stop_words_set = set(line.strip() for line in f if line.strip())

        # ãƒ­ãƒ¼ã‚«ãƒ«è¾æ›¸ãŒãªã‘ã‚Œã°ãƒ“ãƒ«ãƒ‰
        custom_dict_path = "custom_dict/user.dic"
        if not os.path.exists(custom_dict_path):
            print("ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ“ãƒ«ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            build_user_dic_from_local_file(
                entry_csv_path="custom_dict/user_entry.csv",
                dic_dir="/usr/share/mecab/dic/ipadic",
                output_dir="custom_dict",
            )

    # å¿…é ˆã®ç’°å¢ƒå¤‰æ•°ãŒæƒã£ã¦ã„ã‚‹ã‹ç¢ºèª
    if not all([NOTION_TOKEN, DATABASE_ID, GOOGLE_CREDENTIALS_JSON]):
        raise ValueError("ç’°å¢ƒå¤‰æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # Notionã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    all_text: str = fetch_good_things(NOTION_TOKEN, DATABASE_ID, DAY_LIMIT)

    # Taggerã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
    print("ğŸ”µ get_tagger ã‚’å‘¼ã³å‡ºã—ã¾ã™")
    tagger = get_tagger(custom_dict_path)

    # å½¢æ…‹ç´ è§£æã¨é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
    word_count: Counter = analyse_word(all_text, tagger, stop_words_set)
    print(word_count.most_common(TOP_N))

    # é–‹ç™ºæ™‚ã®ã¿ã‚°ãƒ©ãƒ•ã‚’å‡ºåŠ›
    if not IS_RENDER:
        fig = generate_bar_chart(word_count)
        kaleido.get_chrome_sync()
        fig.write_image("output/keyword_chart.png")
        print("ã‚°ãƒ©ãƒ•ã®å‡ºåŠ›ãŒå®Œäº†ã—ã¾ã—ãŸ")

    return word_count


if __name__ == "__main__":
    run_keyword_extraction()

import base64
import os
import tempfile
from collections import Counter

import streamlit as st
from dotenv import load_dotenv
from notion_handler import fetch_good_things
from sheets_writer import connect_to_sheet, write_word_count
from utils.supabase import get_supabase_client
from word_analyser import analyse_word

TOP_N = 5  # é »å‡ºå˜èªã®ä¸Šä½ã‹ã‚‰æ•°ãˆã¦ä½•å€‹ã‚’è¡¨ç¤ºã™ã‚‹ã‹
DAY_LINIT = 30  # éå»ä½•æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹


def main() -> Counter:
    if os.getenv("RENDER") == "true":
        # Renderç’°å¢ƒï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã—ã€jsonã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼‰
        NOTION_TOKEN = os.getenv("NOTION_TOKEN")
        DATABASE_ID = os.getenv("DATABASE_ID")

        b64_creds = os.getenv("GOOGLE_CREDENTIALS_JSON")
        if not b64_creds:
            raise ValueError("GOOGLE_CREDENTIALS_JSON ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        decoded = base64.b64decode(b64_creds)
        with tempfile.NamedTemporaryFile(
            mode="w+b", delete=False, suffix=".json"
        ) as tmp:
            tmp.write(decoded)
            GOOGLE_CREDENTIALS_JSON = tmp.name  # ğŸ‘ˆ tempãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ã‚»ãƒƒãƒˆ
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
        load_dotenv(dotenv_path="config/.env")
        NOTION_TOKEN = os.getenv("NOTION_TOKEN")
        DATABASE_ID = os.getenv("DATABASE_ID")
        GOOGLE_CREDENTIALS_JSON = os.getenv("GOOGLE_CREDENTIALS_PATH")

    supabase = get_supabase_client()

    # å¿…é ˆã®ç’°å¢ƒå¤‰æ•°ãŒæƒã£ã¦ã„ã‚‹ã‹ç¢ºèª
    if not all([NOTION_TOKEN, DATABASE_ID, GOOGLE_CREDENTIALS_JSON]):
        raise ValueError("ç’°å¢ƒå¤‰æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # Notionã‹ã‚‰ã€Œè‰¯ã‹ã£ãŸã“ã¨1ã€œ3ã€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºãƒ»çµåˆ
    all_text: str = fetch_good_things(NOTION_TOKEN, DATABASE_ID, DAY_LINIT)

    # supabaseãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
    user_id = st.session_state.user.id
    response = (
        supabase.table("stop_words").select("word").eq("user_id", user_id).execute()
    )

    stop_words_set: set[str] = set(item["word"] for item in response.data)

    # å˜èªã®å‡ºç¾é »åº¦ã‚’è§£æ
    word_count: Counter = analyse_word(all_text, "custom_dict/user.dic", stop_words_set)

    # Google Sheetsã«æ¥ç¶šï¼ˆæŒ‡å®šã—ãŸã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåã‚’é–‹ãï¼‰
    worksheet = connect_to_sheet(GOOGLE_CREDENTIALS_JSON, "Keyword Extraction")

    # é »å‡ºå˜èªã¨ãã®å‡ºç¾å›æ•°ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€
    write_word_count(worksheet, word_count, TOP_N)

    return word_count
